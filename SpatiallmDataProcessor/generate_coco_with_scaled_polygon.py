import json
import numpy as np
import os
import logging
import tqdm
import time
import glob
import pandas as pd
import open3d as o3d
from shapely import Polygon
from collections import defaultdict
from functools import partial
import cv2
import math
from multiprocessing import Pool, cpu_count

from ply_2d_projection_with_txt_annotations import parse_annotation

def generate_2d_point_cloud_density_map(points, output_png_path, min_xy, range_xy, target_size=(256, 256)):
    """ä½¿ç”¨ç»Ÿä¸€è®¡ç®—çš„min_xyå’Œrange_xyç”Ÿæˆå¯†åº¦å›¾"""
    if points.shape[1] == 3:
        points = points[:, :2]

    eps = 1e-6
    range_xy = np.where(range_xy < eps, eps, range_xy)

    normalized_xy = (points - min_xy) / range_xy
    pixel_coords = (normalized_xy * (np.array(target_size) - 1)).astype(np.int32)

    density = np.zeros(target_size, dtype=np.float32)
    h, w = target_size

    valid = (
        (pixel_coords[:, 0] >= 0) & (pixel_coords[:, 0] < w) &
        (pixel_coords[:, 1] >= 0) & (pixel_coords[:, 1] < h)
    )
    valid_coords = pixel_coords[valid]
    unique_coords, counts = np.unique(valid_coords, axis=0, return_counts=True)

    density[unique_coords[:, 1], unique_coords[:, 0]] = counts
    density_normalized = density / (np.max(density) + 1e-6)
    density_uint8 = (density_normalized * 255).astype(np.uint8)
    cv2.imwrite(output_png_path, density_uint8)
    return density_uint8


def calculate_unified_scaling_params(point_coords, annotation_coords, target_size=(256, 256), padding=0.1):
    """
    åˆå¹¶ç‚¹äº‘å’Œæ ‡æ³¨åæ ‡è®¡ç®—ç»Ÿä¸€çš„ç¼©æ”¾å‚æ•°
    point_coords: ç‚¹äº‘åæ ‡ (Nx2 numpy array)
    annotation_coords: æ ‡æ³¨åæ ‡ (Mx2 numpy array)
    è¿”å›: min_xy, range_xy, target_size
    """
    # åˆå¹¶æ‰€æœ‰åæ ‡
    all_coords = np.vstack([point_coords, annotation_coords])
    
    min_xy = np.min(all_coords, axis=0)
    max_xy = np.max(all_coords, axis=0)
    range_xy = max_xy - min_xy
    
    # æ‰©å±•è¾¹ç•Œ
    max_xy = max_xy + padding * range_xy
    min_xy = min_xy - padding * range_xy
    range_xy = max_xy - min_xy
    
    eps = 1e-6
    range_xy = np.where(range_xy < eps, eps, range_xy)
    
    return min_xy, range_xy, target_size


def normalize_vector(x, y):
    length = math.hypot(x, y)
    if length == 0:
        return (0.0, 0.0)  # é¿å…é™¤é›¶ï¼Œä½†æ­£å¸¸æƒ…å†µä¸‹ wall ä¸ä¼šæ˜¯é›¶é•¿
    return (x / length, y / length)

def process_single_scene(scene_data, split_csv_path, ply_data_root, layout_data_root, ply_output_dir, layout_output_dir, target_size):
    """å¤„ç†å•ä¸ªåœºæ™¯ï¼Œè¿”å›ç»“æœå­—å…¸"""
    
    df = pd.read_csv(split_csv_path, sep=',', skipinitialspace=True)
    df.columns = [c.strip() for c in df.columns]

    scene_id, chunk_ids = scene_data
    result = {
        'scene_id': scene_id,
        'success': False,
        'error': None,
        'chunks_processed': 0,
        'files_processed': 0,
        'warn_messages': []
    }
    
    try:
        all_points = []
        all_annotation_coords = []  # æ”¶é›†æ‰€æœ‰æ ‡æ³¨åæ ‡
        
        # 1. æ”¶é›†ç‚¹äº‘æ•°æ®
        for chunk_id in chunk_ids:
            chunk_folder = os.path.join(ply_data_root, f"chunk_{str(chunk_id).zfill(3)}")
            if not os.path.exists(chunk_folder):
                result['warn_messages'].append(f"chunkæ–‡ä»¶å¤¹ä¸å­˜åœ¨: {chunk_folder}")
                continue
            
            pattern = f"{scene_id}_*_{SAMPLE_ID}.ply"
            ply_files = glob.glob(os.path.join(chunk_folder, pattern))
            
            if not ply_files:
                result['warn_messages'].append(f"æœªæ‰¾åˆ°PLYæ–‡ä»¶: {chunk_folder}/{pattern}")
                continue
            
            result['chunks_processed'] += 1
            
            for ply_path in ply_files:
                try:
                    pcd = o3d.io.read_point_cloud(ply_path)
                    if pcd.has_points():
                        points = np.asarray(pcd.points)
                        all_points.append(points[:, :2])
                        result['files_processed'] += 1
                    else:
                        result['warn_messages'].append(f"ç©ºç‚¹äº‘: {ply_path}")
                except Exception as e:
                    result['warn_messages'].append(f"è¯»å–å¤±è´¥ {ply_path}: {e}")
                    continue
        
        if not all_points:
            result['error'] = "æ²¡æœ‰æœ‰æ•ˆçš„ç‚¹äº‘æ•°æ®"
            return result
        
        all_points = np.vstack(all_points)
        
        # 2. æ”¶é›†æ ‡æ³¨æ•°æ®åæ ‡
        all_txt_files = []
        for chunk_id in chunk_ids:
            chunk_folder = os.path.join(layout_data_root, f"chunk_{str(chunk_id).zfill(3)}")
            if not os.path.exists(chunk_folder):
                result['warn_messages'].append(f"chunkæ–‡ä»¶å¤¹ä¸å­˜åœ¨: {chunk_folder}")
                continue
            
            pattern = f"{scene_id}_*_{SAMPLE_ID}.txt"
            txt_files = glob.glob(os.path.join(chunk_folder, pattern))
            
            if not txt_files:
                result['warn_messages'].append(f"æœªæ‰¾åˆ°txtæ–‡ä»¶: {chunk_folder}/{pattern}")
                continue
            
            all_txt_files.extend(txt_files)

        logging.info(f"[DEBUG] scene_id={scene_id}, layout_data_root={layout_data_root}")
        logging.info(f"[DEBUG] æ‰€æœ‰æ‰¾åˆ°çš„ txt_files: {all_txt_files}")
        
        # ä¸´æ—¶å­˜å‚¨æ ‡æ³¨åæ ‡ç”¨äºè®¡ç®—ç»Ÿä¸€èŒƒå›´
        temp_annotation_coords = []
        temp_entities_list = []  # å­˜å‚¨æ‰€æœ‰å®ä½“ç”¨äºåç»­å¤„ç†
        
        for txt_path in all_txt_files:
            filename = os.path.basename(txt_path)
            if filename in TXT_REPAIRED:
                repaired_txt_path = os.path.join("poly_repair_output/txt_repaired/", filename)
                if os.path.exists(repaired_txt_path):
                    txt_path = repaired_txt_path
                    result['warn_messages'].append(f"âœ… ä½¿ç”¨ä¿®å¤åçš„æ–‡ä»¶ {filename}")
                    print(f'âœ… ä½¿ç”¨ä¿®å¤åçš„æ–‡ä»¶ {filename}')
                else:
                    result['warn_messages'].append(f"âš ï¸ ä¿®å¤æ–‡ä»¶ä¸å­˜åœ¨")

            filename_without_ext = os.path.splitext(filename)[0]
            row = df.loc[df['id'] == filename_without_ext]
            if row.empty:
                result['warn_messages'].append(f"æœªæ‰¾åˆ°å¯¹åº”çš„è¡Œ: {txt_path}")
                continue
            
            room_id = int(row['room_id'].iloc[0])
            entities = parse_annotation(txt_path, room_id)
            temp_entities_list.append((entities, room_id, row))  # ä¿å­˜å®ä½“å’Œç›¸å…³ä¿¡æ¯
            
            # æ”¶é›†å¢™çš„åæ ‡
            if entities["walls"]:
                for wall in entities["walls"]:
                    temp_annotation_coords.append([wall.ax, wall.ay])
                    temp_annotation_coords.append([wall.bx, wall.by])
            
            # æ”¶é›†é—¨çš„åæ ‡
            if entities["doors"]:
                for door in entities["doors"]:
                    temp_annotation_coords.append([door.position_x, door.position_y])
            
            # æ”¶é›†çª—çš„åæ ‡
            if entities["windows"]:
                for window in entities["windows"]:
                    temp_annotation_coords.append([window.position_x, window.position_y])
        
        # 3. è®¡ç®—ç»Ÿä¸€çš„ç¼©æ”¾å‚æ•°
        if not temp_annotation_coords:
            result['warn_messages'].append("æœªæ‰¾åˆ°ä»»ä½•æ ‡æ³¨åæ ‡ï¼Œå°†ä»…ä½¿ç”¨ç‚¹äº‘èŒƒå›´")
            annotation_coords = np.array([[0, 0]])  # ç”¨ä¸€ä¸ªä¸´æ—¶ç‚¹é¿å…ç©ºæ•°ç»„
        else:
            annotation_coords = np.array(temp_annotation_coords)
        
        # è®¡ç®—ç»Ÿä¸€ç¼©æ”¾å‚æ•°
        min_xy, range_xy, target_size = calculate_unified_scaling_params(
            all_points, 
            annotation_coords, 
            target_size
        )
        H, W = target_size
        
        # 4. ç”Ÿæˆç‚¹äº‘å¯†åº¦å›¾ï¼ˆä½¿ç”¨ç»Ÿä¸€ç¼©æ”¾å‚æ•°ï¼‰
        output_png = os.path.join(ply_output_dir, f"{scene_id}.png")
        generate_2d_point_cloud_density_map(all_points, output_png, min_xy, range_xy, target_size)
        
        # å®šä¹‰ç¼©æ”¾å‡½æ•°
        def scale_world_to_pixel(world_coords):
            world_coords = np.asarray(world_coords, dtype=np.float32)
            normalized = (world_coords - min_xy) / range_xy
            pixel_coords = (normalized * (np.array(target_size) - 1)).astype(np.int32)
            return pixel_coords
        
        # 5. å¤„ç†æ ‡æ³¨æ•°æ®ï¼ˆä½¿ç”¨ç»Ÿä¸€ç¼©æ”¾å‚æ•°ï¼‰
        annos = []
        for entities, room_id, row in temp_entities_list:
            room_type = row['room_type'].iloc[0]
            img_id = int(scene_id.split('_')[1])
            category_id = CATEGORIES_NAME_TO_ID[room_type]

            # å¤„ç†å¢™ä½“
            if entities["walls"]:
                wall_vertices = []
                for wall in entities["walls"]:
                    wall_vertices.append([wall.ax, wall.ay])
                    wall_vertices.append([wall.bx, wall.by])
                
                scaled_wall_vertices = scale_world_to_pixel(wall_vertices)
                wall_polygon_points = scaled_wall_vertices[::2]
                wall_polygon = Polygon(wall_polygon_points)
                
                scaled_segmentation = []
                for point in wall_polygon_points:
                    x, y = point
                    scaled_segmentation.extend([float(x), float(y)])
                
                x_coords = scaled_wall_vertices[:, 0]
                y_coords = scaled_wall_vertices[:, 1]
                x_min, x_max = x_coords.min(), x_coords.max()
                y_min, y_max = y_coords.min(), y_coords.max()
                
                expand = 2
                bbox = [
                    float(max(0, x_min - expand)),
                    float(max(0, y_min - expand)),
                    float(min(W, x_max - x_min + 2*expand)),
                    float(min(H, y_max - y_min + 2*expand))
                ]
                
                wall_anno = {
                    "room_id": room_id,
                    "segmentation": [scaled_segmentation],
                    "area": float(wall_polygon.area),
                    "iscrowd": 0,
                    "image_id": img_id,
                    "bbox": bbox,
                    "category_id": category_id,
                    "id": None
                }
                annos.append(wall_anno)
            
            # å¤„ç†é—¨å’Œçª—
            def find_wall_by_id(walls, wall_id):
                for wall in walls:
                    if wall.id == wall_id:
                        return wall
                return None
            
            def process_line_entity(entity, entities, category_name):
                wall = find_wall_by_id(entities["walls"], entity.wall_id)
                if not wall:
                    logging.info(f"è­¦å‘Šï¼š{category_name} (id={entity.id}) æ‰¾ä¸åˆ°å¯¹åº”çš„ wall (wall_id={entity.wall_id})")
                    return None
                
                cx, cy = entity.position_x, entity.position_y
                w = entity.width
                
                dir_x = wall.bx - wall.ax
                dir_y = wall.by - wall.ay
                dir_norm_x, dir_norm_y = normalize_vector(dir_x, dir_y)
                
                half_w = w / 2
                x1_world = cx - dir_norm_x * half_w
                y1_world = cy - dir_norm_y * half_w
                x2_world = cx + dir_norm_x * half_w
                y2_world = cy + dir_norm_y * half_w
                
                endpoints = np.array([[x1_world, y1_world], [x2_world, y2_world]])
                scaled_endpoints = scale_world_to_pixel(endpoints)
                x1, y1 = scaled_endpoints[0]
                x2, y2 = scaled_endpoints[1]
                
                segmentation = [float(x1), float(y1), float(x2), float(y2)]
                x_min, x_max = min(x1, x2), max(x1, x2)
                y_min, y_max = min(y1, y2), max(y1, y2)
                bbox = [float(x_min), float(y_min), float(x_max - x_min), float(y_max - y_min)]
                
                return {
                    "room_id": entity.room_id,
                    "segmentation": [segmentation],
                    "area": 0.0,
                    "iscrowd": 0,
                    "image_id": img_id,
                    "bbox": bbox,
                    "category_id": CATEGORIES_NAME_TO_ID[category_name],
                    "id": None
                }
            
            if entities["doors"]:
                for door in entities["doors"]:
                    door_anno = process_line_entity(door, entities, "door")
                    if door_anno:
                        annos.append(door_anno)
            
            if entities["windows"]:
                for window in entities["windows"]:
                    window_anno = process_line_entity(window, entities, "window")
                    if window_anno:
                        annos.append(window_anno)

        # æ·»åŠ annotation id
        for idx, anno in enumerate(annos):
            anno["id"] = idx
        
        scene_coco_data = {
            "images": [
                {
                    "file_name": f"{scene_id}.png",
                    "id": scene_id.split("_")[1],
                    "width": W,
                    "height": H
                }
            ],
            "annotations": annos,
            "categories": []
        }
        
        # å†™å…¥JSON
        try:
            logging.info(f'line323: scene_coco_data[{scene_id}]')
            logging.info(scene_coco_data)
            
            json_path = os.path.join(layout_output_dir, f"{scene_id}.json")
            logging.info(f"æ­£åœ¨å†™å…¥ JSON æ–‡ä»¶: {json_path}")
            with open(json_path, "w", encoding='utf-8') as f:
                json.dump(scene_coco_data, f, ensure_ascii=False, indent=2)
            logging.info(f"âœ… JSON æ–‡ä»¶å†™å…¥æˆåŠŸ: {json_path}")
            
            result['success'] = True

        except Exception as e:
            result['error'] = f"å†™å…¥ JSON æ–‡ä»¶å¤±è´¥: {str(e)}"
            logging.error(f"[ERROR] å†™å…¥ JSON å¤±è´¥: {e}")

    except Exception as e:
        result['error'] = f"æœªé¢„æœŸé”™è¯¯: {str(e)}"
    
    return result


def batch_generate_coco_scaled_parallel(    
    split_csv_path="split_sample_sample_id.csv",
    ply_data_root="data",
    layout_data_root="layout",
    ply_output_dir="ply_output_dir",
    layout_output_dir="layout_output_dir",
    target_size=(256, 256),
    num_workers=None
):
    # 1. è¯»å–CSV
    if not os.path.exists(split_csv_path):
        logging.error(f"split.csvä¸å­˜åœ¨: {split_csv_path}")
        return
    
    df = pd.read_csv(split_csv_path, sep=',', skipinitialspace=True)
    df.columns = [c.strip() for c in df.columns]
    
    scene_to_chunks = defaultdict(list)
    for _, row in df.iterrows():
        if pd.notna(row['scene_id']) and pd.notna(row['chunk_id']):
            scene_to_chunks[row['scene_id']].append(int(row['chunk_id']))
    
    for scene_id in scene_to_chunks:
        scene_to_chunks[scene_id] = list(set(scene_to_chunks[scene_id]))
    
    total_scenes = len(scene_to_chunks)
    logging.info(f"å‘ç° {total_scenes} ä¸ªåœºæ™¯")
    
    # 2. å‡†å¤‡è¾“å‡ºç›®å½•
    os.makedirs(ply_output_dir, exist_ok=True)
    os.makedirs(layout_output_dir, exist_ok=True)

    
    # 3. é…ç½®è¿›ç¨‹æ•°
    if num_workers is None:
        num_workers = max(1, int(cpu_count() * 1.5))
    logging.info(f"å¯åŠ¨ {num_workers} ä¸ªè¿›ç¨‹å¹¶è¡Œå¤„ç†")
    
    # 4. åˆ›å»ºè¿›ç¨‹æ± 
    process_func = partial(
        process_single_scene,
        ply_data_root=ply_data_root,
        split_csv_path=split_csv_path,
        layout_data_root=layout_data_root,
        ply_output_dir=ply_output_dir,
        layout_output_dir=layout_output_dir,
        target_size=target_size
    )
    
    scene_items = list(scene_to_chunks.items())
    
    # 5. å¹¶è¡Œå¤„ç†
    with Pool(processes=num_workers) as pool:
        results = list(
            tqdm.tqdm(
                pool.imap(process_func, scene_items),
                total=len(scene_items),
                desc="å¤„ç†è¿›åº¦"
            )
        )
    
    # 6. ç»Ÿä¸€è®°å½•æ—¥å¿—
    success_count = 0
    for result in results:
        scene_id = result['scene_id']
        
        for warn in result['warn_messages']:
            logging.warning(f"[{scene_id}] {warn}")
        
        if result['success']:
            success_count += 1
            logging.info(
                f"âœ… åœºæ™¯ {scene_id} æˆåŠŸ: "
                f"å¤„ç†chunks={result['chunks_processed']}, "
                f"æ–‡ä»¶={result['files_processed']}, "
            )
        else:
            logging.error(f"âŒ åœºæ™¯ {scene_id} å¤±è´¥: {result['error']}")
    
    logging.info(f"ğŸ‰ å®Œæˆï¼æˆåŠŸå¤„ç† {success_count}/{total_scenes} ä¸ªåœºæ™¯")


def run_for_params(sample_id, img_size):
    global SAMPLE_ID
    SAMPLE_ID = sample_id
    IMG_SIZE = img_size
    ply_data_root = "/mnt/data3/spatial_dataset/pcd" 
    layout_data_root = "/mnt/data3/spatial_dataset/layout"
    # ply_data_root = "data/pcd"
    # layout_data_root = "data/layout"

    ply_output_dir = f"coco_with_scaled/sample{SAMPLE_ID}_{IMG_SIZE}/density_map"
    layout_output_dir = f"coco_with_scaled/sample{SAMPLE_ID}_{IMG_SIZE}/anno"
    log_path = f'coco_with_scaled/log/sample{SAMPLE_ID}_{IMG_SIZE}.log'
    split_csv_path = f"data/csv/split_by_sample/split_sample_{SAMPLE_ID}.csv"

    os.makedirs(os.path.dirname(ply_output_dir), exist_ok=True)
    os.makedirs(os.path.dirname(layout_output_dir), exist_ok=True)
    os.makedirs(os.path.dirname(log_path), exist_ok=True)

    NUM_WORKERS = 12

    logging.basicConfig(
        filename=log_path,
        level=logging.INFO,
        format='%(asctime)s | %(levelname)s | %(message)s',
        filemode='w'
    )
    logging.info(f"Starting for SAMPLE_ID={SAMPLE_ID}, IMG_SIZE={IMG_SIZE}")

    batch_generate_coco_scaled_parallel(
        split_csv_path=split_csv_path,
        ply_data_root=ply_data_root,
        layout_data_root=layout_data_root,
        ply_output_dir=ply_output_dir,
        layout_output_dir=layout_output_dir,
        target_size=(IMG_SIZE, IMG_SIZE),
        num_workers=NUM_WORKERS
    )
    logging.info(f"Finished for SAMPLE_ID={SAMPLE_ID}, IMG_SIZE={IMG_SIZE}")

if __name__ == "__main__":
    # è¯»å–categories.json
    file_path = 'categories.json'
    with open(file_path, 'r', encoding='utf-8') as f:
        categories_json = json.load(f)
    categories = categories_json.get('categories', [])
    CATEGORIES = categories
    CATEGORIES_NAME_TO_ID = {}
    for category in categories:
        name = category.get('name')
        cat_id = category.get('id')
        if name is not None and cat_id is not None:
            CATEGORIES_NAME_TO_ID[name] = cat_id

    txt_poly_repaired_path = 'poly_repair_output/repaired_files_mapping.csv'
    
    def get_files_to_repair_list(repaired_mapping_csv_path):
        files_to_repair = []

        if not os.path.exists(repaired_mapping_csv_path):
            print(f"[è­¦å‘Š] ä¿®å¤æ–‡ä»¶æ˜ å°„ CSV ä¸å­˜åœ¨: {repaired_mapping_csv_path}")
            return files_to_repair

        try:
            df = pd.read_csv(repaired_mapping_csv_path, sep=',', skipinitialspace=True)
            files_col = df.iloc[:, 1]

            for filename in files_col:
                if pd.notna(filename) and isinstance(filename, str):
                    files_to_repair.append(filename.strip())
        except Exception as e:
            print(f"[é”™è¯¯] è¯»å–ä¿®å¤æ–‡ä»¶åˆ—è¡¨å¤±è´¥: {e}")

        return files_to_repair

    TXT_REPAIRED = get_files_to_repair_list(txt_poly_repaired_path)

    PARAM_COMBINATIONS = [
        {"sample_id": 0, "img_size": 256},
        {"sample_id": 0, "img_size": 1024},
        {"sample_id": 1, "img_size": 256},
        {"sample_id": 1, "img_size": 1024},
        {"sample_id": 2, "img_size": 256},
        {"sample_id": 2, "img_size": 1024},
        {"sample_id": 3, "img_size": 256},
        {"sample_id": 3, "img_size": 1024},
    ]

    for params in PARAM_COMBINATIONS:
        sample_id = params["sample_id"]
        img_size = params["img_size"]
        print(f"\n{'='*40}")
        print(f"Running for SAMPLE_ID={sample_id}, IMG_SIZE={img_size}")
        print(f"{'='*40}")
        run_for_params(sample_id, img_size)